{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85edb55",
   "metadata": {},
   "source": [
    "## 19_march_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79347023",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8d214",
   "metadata": {},
   "source": [
    "* Min-Max scaling is a data preprocessing technique that transforms the values of each feature in a dataset to a range of [0, 1]. \n",
    "\n",
    "* Min-Max scaling is  used to normalize features before training a machine learning model. This is because different features can have different scales, and this can make it difficult for the model to learn. By normalizing the features, the model can learn more effectively.\n",
    "\n",
    "**For example:\n",
    "\n",
    "* Age (in years)\n",
    "* Height (in inches)\n",
    "* Weight (in pounds)\n",
    "\n",
    "The age feature has a range of 0 to 100, the height feature has a range of 36 to 84, and the weight feature has a range of 60 to 300. If we train a machine learning model on this dataset without normalizing the features, the model will have to learn how to deal with the different scales of the features. This can make it difficult for the model to learn, and it can also make the model more sensitive to noise in the data so here we have to use normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b644f02e",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f844ca",
   "metadata": {},
   "source": [
    "* Unit vector scaling is a data preprocessing technique that transforms each feature vector in a dataset to have a unit length. This is done by dividing each feature vector by its Euclidean norm\n",
    "* For example:\n",
    "age feature has a range of 20 to 40, the height feature has a range of 60 to 80, and the weight feature has a range of 150 to 250.we use unit vector scaling to normalize this dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc7b90",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b367f0f",
   "metadata": {},
   "source": [
    "* Principal component analysis (PCA) is a statistical technique that is used to reduce the dimensionality of a dataset while preserving as much information as possible.\n",
    "* PCA is often used in machine learning and data science applications where the dataset is too large to be easily handled or where the dimensionality of the data is a limiting factor. \n",
    "\n",
    "##### example\n",
    "* Consider a dataset of 1000 observations with 100 features. This dataset is too large to be easily handled by most machine learning algorithms. PCA can be used to reduce the dimensionality of this dataset to 10 principal components, which will explain 99% of the variance in the data. This will make the dataset much easier to handle and will also improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c2391",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac80ca",
   "metadata": {},
   "source": [
    "* Principal component analysis (PCA) is a statistical technique that can be used to reduce the dimensionality of a dataset while preserving as much of the variance as possible.\n",
    "* Feature extraction is the process of identifying and extracting the most important features from a dataset\n",
    "* PCA can be used as a feature extraction technique by identifying the principal components of the dataset\n",
    "#### example\n",
    "* consider a dataset of images of faces. Each image can be represented as a vector of pixel values. If the dataset contains a large number of images, the resulting vector space will be high-dimensional. PCA can be used to reduce the dimensionality of this space by identifying the principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578bd701",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6621d0",
   "metadata": {},
   "source": [
    "* Min-Max scaling is a data preprocessing technique which keeps all values in dataset within range of 0 to 1.\n",
    "* in the case of a food delivery service, the price, rating, and delivery time could all be normalized using Min-Max scaling. This would ensure that all three features have the same weight when the recommendation system is trained.\n",
    "* We would  normalize the rating and delivery time. Once all three features have been normalized, we can then train the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d470c98",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58778742",
   "metadata": {},
   "source": [
    "* To use PCA to reduce the dimensionality of a dataset, we first need to calculate the covariance matrix of the data. The covariance matrix is a square matrix that shows the covariance between each pair of features. The covariance of two features is a measure of how much they vary together.\n",
    "\n",
    "* To reduce the dimensionality of the dataset, we can choose the top k principal components, where k is the desired number of dimensions. The data can then be projected onto the subspace spanned by the top k principal components. This will reduce the dimensionality of the data while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59991373",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b04258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956380de",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de8f9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'n':[1,5,10,15,20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3770d7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1f2b1",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516dcee",
   "metadata": {},
   "source": [
    "* The number of principal components to retain depends on the amount of variance you want to explain. In this case, if you want to explain 95% of the variance in the dataset, we would retain the first two principal components.\n",
    "* if we want to explain 99% of the variance in the dataset, you would retain the first three principal components\n",
    "* The choice of how many principal components to retain is a trade-off between the amount of variance explained and the number of dimensions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf67a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
