{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2a0d5e",
   "metadata": {},
   "source": [
    "# 6_April_Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda20a8",
   "metadata": {},
   "source": [
    "### Q1.What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a1e2a",
   "metadata": {},
   "source": [
    "Given a training dataset with input vectors ( x_i ) of dimension ( n ), and their corresponding binary labels ( y_i ) which can take on values of either -1 or 1, the goal of a linear SVM is to find a hyperplane ( w \\cdot x + b = 0 ) that separates the data points of different classes with the largest margin.\n",
    "\n",
    "The decision function of a linear SVM is given by:\n",
    "\n",
    "[ f(x) = w * x + b ]\n",
    "\n",
    "where ( w ) is the weight vector, ( b ) is the bias term, and ( x ) is the input vector.\n",
    "\n",
    "The objective of a linear SVM is to maximize the margin, which is the distance between the hyperplane and the closest data points from each class. Mathematically, this can be formulated as an optimization problem:\n",
    "\n",
    "Minimize: [ \\frac{1}{2} | w |^2 ]\n",
    "\n",
    "subject to the constraints: y_i (w * x_i + b) >= 1 ∀i\n",
    "\n",
    "where ( y_i ) is the binary label of the i-th training sample, ( x_i ) is the i-th input vector, and ( | w |^2 ) represents the squared Euclidean norm of the weight vector.\n",
    "\n",
    "The constraints in the optimization problem ensure that the data points are correctly classified with a margin of at least 1. The optimization problem is typically solved using various techniques such as the Sequential Minimal Optimization (SMO) algorithm or gradient descent methods. Once the optimization problem is solved, the resulting weight vector ( w ) and bias term ( b ) can be used to make predictions on new data points using the decision function ( f(x) )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbfda1",
   "metadata": {},
   "source": [
    "### Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d205d",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is to maximize the margin, which is the distance between the hyperplane and the closest data points from each class, while minimizing the norm of the weight vector. The objective function of a linear SVM is typically formulated as an optimization problem with the goal of finding the optimal weight vector and bias term that defines the hyperplane.\n",
    "\n",
    "Mathematically, the objective function of a linear SVM can be expressed as follows:\n",
    "\n",
    "Minimize: [ \\frac{1}{2} | w |^2 ]\n",
    "\n",
    "subject to the constraints: [ y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i ]\n",
    "\n",
    "where ( w ) is the weight vector, ( b ) is the bias term, ( x_i ) is the i-th input vector in the training dataset, and ( y_i ) is the binary label of the i-th training sample. The constraint ( y_i (w \\cdot x_i + b) \\geq 1 ) ensures that all data points are correctly classified with a margin of at least 1, where ( y_i ) represents the binary label of the i-th training sample and ( w \\cdot x_i + b ) is the decision function of the linear SVM.\n",
    "\n",
    "The objective function of a linear SVM seeks to find the weight vector ( w ) and bias term ( b ) that satisfy the constraints and minimize the norm of ( w ), which represents the regularization term. The regularization term helps to prevent overfitting and promotes a simpler model with a larger margin, which is desirable in SVMs for better generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d9bd3",
   "metadata": {},
   "source": [
    "### Q3.What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7719b1",
   "metadata": {},
   "source": [
    "kernel trick in Support Vector Machines (SVM) is a technique that allows SVMs to efficiently handle nonlinearly separable data by transforming the input data into a higher-dimensional space using a kernel function, without explicitly computing the coordinates of the transformed data points. This allows SVMs to model complex decision boundaries that are not possible in the original input space.\n",
    "\n",
    "In a linear SVM, the decision boundary is a hyperplane that separates the data points of different classes. However, in many real-world scenarios, the data points may not be linearly separable, and a linear SVM may not be able to achieve good classification performance. The kernel trick addresses this limitation by implicitly mapping the input data points to a higher-dimensional feature space, where they may become linearly separable, and then applying a linear SVM in this transformed space.\n",
    "\n",
    "in the linear SVM formulation with a kernel function which computes the similarity or inner product of the data points in the higher-dimensional space. This allows the SVM to implicitly learn a nonlinear decision boundary in the original input space, without explicitly transforming the data into the higher-dimensional space.it satisfies Mercer's condition, which ensures that the kernel function corresponds to a valid positive semi-definite kernel matrix. Commonly used kernel functions in SVMs include the linear kernel, polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel, among others. The choice of kernel function depends on the characteristics of the data and the specific problem at hand.\n",
    "\n",
    "The kernel trick in SVMs provides a powerful way to handle nonlinearly separable data and enables SVMs to model complex decision boundaries, making SVMs a popular choice for classification and regression tasks in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f28425",
   "metadata": {},
   "source": [
    "### Q4.What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b9bcb",
   "metadata": {},
   "source": [
    "Support vectors play a critical role in Support Vector Machines (SVM) as they are the data points that lie closest to the decision boundary (hyperplane) and determine the position and orientation of the hyperplane. Support vectors are the data points that have non-zero coefficients in the SVM's solution, and they are used to define the margin and determine the classification of new data points.\n",
    "\n",
    "In an SVM, the objective is to find a hyperplane that separates the data points of different classes with the largest margin. The support vectors are the data points that are closest to the hyperplane, and they are the ones that determine the position and orientation of the hyperplane. The support vectors are the data points that have non-zero coefficients in the SVM's solution, meaning that they contribute to the definition of the decision function.\n",
    "\n",
    "Support vectors are critical because they are the data points that are most likely to be misclassified or lie close to the decision boundary. These are the data points that are most challenging to classify and have the highest potential to affect the performance of the SVM. By using support vectors, SVMs are able to achieve a sparse representation of the decision boundary, as only a subset of the data points are used in defining the decision function.\n",
    "\n",
    "In this example, the circles represent positive class data points, and the crosses represent negative class data points. The decision boundary (hyperplane) learned by the SVM is shown as a dashed line.\n",
    "\n",
    "The support vectors are the data points that lie closest to the decision boundary, which are circled in the figure. These support vectors are critical for defining the position and orientation of the decision boundary. The margin of the SVM is determined by the distance between the decision boundary and the closest support vectors from each class. The SVM aims to maximize this margin during the training process.\n",
    "\n",
    "During prediction, the position of a new data point is determined by its relationship to the decision function and the support vectors. If a new data point lies on the same side of the decision boundary as the majority of the support vectors, it will be classified as the same class as the majority of the support vectors. If it lies on the opposite side of the decision boundary, it will be classified as the opposite class.\n",
    "\n",
    "Support vectors are crucial in SVMs as they determine the position and orientation of the decision boundary, contribute to the definition of the decision function, and play a key role in achieving a sparse representation of the decision boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4b5b1",
   "metadata": {},
   "source": [
    "### Q5.Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dba6a2",
   "metadata": {},
   "source": [
    "Sure! Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM using examples and graphs.\n",
    "\n",
    "Hyperplane: In SVM, a hyperplane is a decision boundary that separates data points of different classes in the input space. In a binary classification problem, a hyperplane is a line in a two-dimensional space, a plane in a three-dimensional space, and a hyperplane in higher-dimensional spaces.\n",
    "\n",
    "Example 2: Non-linearly separable data with a hyperplane A hyperplane (a line in this case) that separates the positive and negative class data points is not possible in the original input space. However, with the use of a kernel function, the data can be transformed into a higher-dimensional space where it becomes linearly separable, and a hyperplane can be found to separate the data points in that space.\n",
    "\n",
    "Marginal plane: In SVM, the marginal plane refers to the plane that is parallel to the hyperplane and touches the support vectors. It is used to define the margin, which is the distance between the hyperplane and the marginal plane.\n",
    "\n",
    "Soft margin: In SVM, a soft margin allows for some misclassification of data points in order to find a better trade-off between overfitting and underfitting. It introduces a penalty for misclassification in the objective function, allowing for a more flexible decision boundary. A soft margin SVM allows for some misclassification of data points. The margin is relaxed to allow for a few data points to fall into the margin or even cross the margin into the opposite class region. The objective function of the SVM is modified to include a penalty for misclassification, and the hyperplane is adjusted to find a better trade-off between misclassification and margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35655ac",
   "metadata": {},
   "source": [
    "### Q6.SVM Implementation through Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a33c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1e56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065527cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iris.data[:,:2]\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46bf2bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446fe83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardize the features\n",
    "scaler=StandardScaler()\n",
    "x_train=scaler.fit_transform(x_train)\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3526ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b772ee09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6afb2e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm.predict(x_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1d58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
