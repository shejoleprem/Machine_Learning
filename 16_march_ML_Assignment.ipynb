{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed507ed",
   "metadata": {},
   "source": [
    "## 16_march_ML_Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15b924",
   "metadata": {},
   "source": [
    "#### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d172c1",
   "metadata": {},
   "source": [
    "Overfitting: ML model fits the training data too closely and does not generalize well to new data.\n",
    "Overfitting can lead to high variance and low bias in the model.\n",
    "\n",
    "Underfitting:the model has high bias and low variance, and it may not perform well on both the training and test data.\n",
    "\n",
    "The consequences of overfitting and underfitting can lead to poor model performance, inaccurate predictions\n",
    "\n",
    "To mitigate overfitting:\n",
    "\n",
    "* Regularization\n",
    "* Dropout\n",
    "\n",
    "To mitigate underfitting:\n",
    "\n",
    "* Adding more features\n",
    "* Reducing regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb04638",
   "metadata": {},
   "source": [
    "#### Q2.How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbf758",
   "metadata": {},
   "source": [
    "There various techniques to reduce overfitting\n",
    "* Regularization: Regularization techniques introduce a penalty term in the loss function that prevents the model's weights from becoming too large. This helps to control the complexity of the model and prevent overfitting.Common regularization techniques include L1, L2, and Elastic Net regularization\n",
    "\n",
    "* Cross-validation: Cross-validation is a technique used to evaluate the performance of a model on multiple splits of the data. By validating the model on multiple subsets of the data, it becomes less likely that the model will overfit to a particular subset.\n",
    "\n",
    "* Early stopping: Early stopping is a technique used to stop the training process once the validation error stops improving. This helps to prevent the model from overfitting to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af326e",
   "metadata": {},
   "source": [
    "#### Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b479be",
   "metadata": {},
   "source": [
    "* Insufficient training data: When there is not enough training data to capture the underlying patterns in the data, the model may not be able to learn the correct patterns and will underfit.\n",
    "\n",
    "* Over-regularization: Over-regularization refers to using too much regularization, which can prevent the model from learning the underlying patterns in the data.\n",
    "\n",
    "* Simplistic model architecture: If the model architecture is too simple and lacks the capacity to capture the complexity of the data, it will underfit.\n",
    "\n",
    "* Poor feature selection: If the features used to train the model are not informative or do not capture the relevant patterns in the data, the model will underfit.\n",
    "\n",
    "* Inappropriate model complexity: If the model complexity is too low for the complexity of the data, the model will underfit.\n",
    "\n",
    "* High noise levels: If there is a lot of noise in the data, it can be difficult for the model to distinguish the signal from the noise, resulting in underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c797f",
   "metadata": {},
   "source": [
    "#### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c30e48",
   "metadata": {},
   "source": [
    "* The bias-variance tradeoff describes the relationship between the complexity of a model and its ability to generalize to new data.\n",
    "* Bias refers to the error that is introduced by the simplifying assumptions made by the model to make the target function easier to approximate\n",
    "* Variance refers to the error that is introduced by the model's sensitivity to small fluctuations in the training data.\n",
    "* tradeoff between bias and variance can be visualized as a U-shaped curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0da72d",
   "metadata": {},
   "source": [
    "#### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55adb8ee",
   "metadata": {},
   "source": [
    "* Train/Validation/Test Split: Splitting the dataset into training, validation, and test sets is a common method for detecting overfitting and underfitting. The training set is used to train the model, the validation set is used to tune the model hyperparameters, and the test set is used to evaluate the model's performance on new data. If the model has high accuracy on the training set but low accuracy on the test set, it is likely overfitting.\n",
    "\n",
    "\n",
    "* Cross-Validation: Cross-validation is a technique for assessing model performance that involves partitioning the data into k equal-sized subsets. The model is trained on k-1 subsets and evaluated on the remaining subset. This process is repeated k times, with each subset used exactly once as the validation data. If the model has high accuracy on the training set but low accuracy on the validation set, it is likely overfitting\n",
    "\n",
    "\n",
    "\n",
    "* Regularization: Regularization is a technique that introduces a penalty term to the loss function to prevent overfitting. L1 and L2 regularization are commonly used to constrain the model's parameters and reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e814a6",
   "metadata": {},
   "source": [
    "#### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high biasand high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec6947",
   "metadata": {},
   "source": [
    "* High bias models are those models that underfit the data, meaning that they are too simple to capture the underlying patterns in the data\n",
    "\n",
    "* example: a linear regression model can be a high bias model when trying to fit a non-linear function.\n",
    "\n",
    "* High variance models are those models that overfit the data and are too complex.\n",
    "\n",
    "* example:  high-degree polynomial regression model might be a high variance model when the training data is limited and noisy.\n",
    "\n",
    "* high bias model might always predict the same class for all images in the dataset\n",
    "\n",
    "* A high variance model that fit the training data too well and fail to generalize to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3740a7",
   "metadata": {},
   "source": [
    "#### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3ec35",
   "metadata": {},
   "source": [
    "* regularization is a technique used to prevent overfitting by adding a penalty term to the model's loss function\n",
    "\n",
    "* L1 regularization:it is a Lasso regularization, adds a penalty term to the loss function that is proportional to the sum of the absolute values of the model's parameters. This has the effect of shrinking some of the parameters to zero, effectively selecting only the most important features for the model.\n",
    "\n",
    "* L2 regularization:it is a Ridge regularization, adds a penalty term to the loss function that is proportional to the square of the sum of the model's parameters. This has the effect of shrinking all of the parameters towards zero, but not necessarily to zero. This can be though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7052b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
