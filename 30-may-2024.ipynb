{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411de574",
   "metadata": {},
   "source": [
    "### Q1 The objectives of Selective Search in R-CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fbc3e",
   "metadata": {},
   "source": [
    "1) to generate region proposals for objects in the image\n",
    "2) to reduce the computation required for classifying every possible window in the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaedcf5",
   "metadata": {},
   "source": [
    "### Q2  Explain the following phases involved in R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ae5ba",
   "metadata": {},
   "source": [
    "1) Region Proposal:- first we have to generate region proposals using selective search\n",
    "2) warping and resizing:- warp the region proposals to a fixed size to feed into the CNN\n",
    "3) Pre-trained CNN Architecture:- use a pretrained CNN like AlexNet to extract features from the warped regions\n",
    "4) Pre-trained SVM model:- Use a pre-trained SVM classifier to classify the features extracted by the CNN into object categories\n",
    "5) clean up: remove duplicate detections using greedy non maximum sppression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6223c2",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049b7ff",
   "metadata": {},
   "source": [
    "pre-trained CNNs that can be used in the pre-trained CNN architecture phase are AlexNet, VGG, ResNet, Inception, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe8fb2",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c997c",
   "metadata": {},
   "source": [
    "In R-CNN, an SVM is trained separately for each object class using the CNN features extracted from warped selective search region proposals as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9127d2",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48b483",
   "metadata": {},
   "source": [
    "a) Take the highest scoring detection\n",
    "\n",
    "b) Remove all other detections that have high intersection-over-union (IoU) with the top detection\n",
    "\n",
    "c) Repeating for the remaining detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704a8184",
   "metadata": {},
   "source": [
    "### Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37a9a8",
   "metadata": {},
   "source": [
    "1) fast rcnn uses single CNN for whole image,reducing computation\n",
    "2) it has higher detection accuracy\n",
    "3) it is much faster at inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9260b",
   "metadata": {},
   "source": [
    "### Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ac61c",
   "metadata": {},
   "source": [
    "1) ROI pooling works by taking the feature maps output by the CNN\n",
    "2) ROI pooling maps the proposed region to a fixed spatial window on the feature maps\n",
    "3) it max pool the values inside each spatial window into a fixed feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c6cc0",
   "metadata": {},
   "source": [
    "### Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45455d",
   "metadata": {},
   "source": [
    "a) ROI Projections: mapping the proposed bounding boxes from the image coordinates to the feature map coordinates\n",
    "\n",
    "b) ROI Pooling: extracting a fixed length feature vector from feature map for each ROI using max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f7e77",
   "metadata": {},
   "source": [
    "### Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89aadbb",
   "metadata": {},
   "source": [
    "In Fast R-CNN, the softmax classifier is used directly on the feature maps output by the ROI pooling, instead of using an SVM. This allows end-to-end training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f18b7f",
   "metadata": {},
   "source": [
    "### Q10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edac48b",
   "metadata": {},
   "source": [
    "* Faster R-CNN replaces Selective Search with a Region Proposal Network (RPN)\n",
    "* The RPN predicts objectness scores and regresses region proposal coordinates in one pass of the CNN\n",
    "* This makes region proposal much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a08e8",
   "metadata": {},
   "source": [
    "### Q11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08df440",
   "metadata": {},
   "source": [
    "An anchor box is a set of defined bounding boxes of different aspect ratios and scales that serve as references for the RPN to predict offsets and objectness scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49569140",
   "metadata": {},
   "source": [
    "### Q12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.datasets import CocoDetection\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as T\n",
    "\n",
    "dataset = CocoDetection(root=\"F:/Third_Year/EDI_2/Sanket-master/assets/coco', annFile='F:/Third_Year/EDI_2/Sanket-master/assets/annotations.json', \n",
    "                        transform=T.ToTensor())\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [len(dataset)-500, 500])\n",
    "\n",
    "backbone = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0)))\n",
    "\n",
    "\n",
    "rpn_head = torchvision.ops.misc.RandomBoxes(anchor_generator)\n",
    "roi_head = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(backbone.out_channels, 91)  \n",
    "\n",
    "\n",
    "model = FasterRCNN(backbone, rpn_head, roi_head)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for images, targets in val_loader:\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses.append(sum(loss for loss in loss_dict.values()))\n",
    "    val_loss = sum(losses) / len(val_loader)\n",
    "    print(f'Epoch: {epoch+1}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "for image in val_dataset:\n",
    "    image = [T.ToTensor()(image)]\n",
    "    output = model(image)[0]\n",
    "    print(f'Found {len(output[\"boxes\"])} objects in image')\n",
    "\n",
    "    img = T.ToPILImage()(image[0])\n",
    "    draw = torchvision.utils.draw_bounding_boxes(img, output[\"boxes\"], colors=\"red\")\n",
    "    draw.save(f\"result.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
